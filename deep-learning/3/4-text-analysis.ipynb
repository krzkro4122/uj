{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To jest przykład tekstu, który potrzebuje normalizacji!  Mamy tu nieco znaków interpunkcyjnych i niepotrzebnych spacji.\n",
      "to jest przykład tekstu który potrzebuje normalizacji mamy tu nieco znaków interpunkcyjnych i niepotrzebnych spacji\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Usuń znaki interpunkcyjne\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Konwersja na małe litery\n",
    "    text = text.lower()\n",
    "\n",
    "    # Usuń nadmiarowe białe znaki\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Przykładowy tekst\n",
    "sample_text = \"To jest przykład tekstu, który potrzebuje normalizacji!  Mamy tu nieco znaków interpunkcyjnych i niepotrzebnych spacji.\"\n",
    "\n",
    "# Normalizacja tekstu\n",
    "normalized_text = normalize_text(sample_text)\n",
    "print(f\"{sample_text}\")\n",
    "print(f\"{normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, casual_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example: NLP involves various preprocessing techniques. For instance, removing stopwords like 'and', 'the', etc., is common.\n",
      "heres example nlp involves various preprocessing techniques instance removing stopwords like etc common\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "sample_text = \"Here's an example: NLP involves various preprocessing techniques. For instance, removing stopwords like 'and', 'the', etc., is common.\"\n",
    "\n",
    "# Cleaning the text\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(sample_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the process of splitting text into words or phrases. It's a crucial step in NLP.\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into', 'words', 'or', 'phrases', '.', \"It's\", 'a', 'crucial', 'step', 'in', 'NLP', '.']\n",
      "['Tokenization', 'is', 'the', 'process', 'of', 'splitting', 'text', 'into', 'words', 'or', 'phrases', '.', 'It', \"'s\", 'a', 'crucial', 'step', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Tokenization is the process of splitting text into words or phrases. It's a crucial step in NLP.\"\n",
    "\n",
    "# Tokenizing the text\n",
    "print(sample_text)\n",
    "print(casual_tokenize(sample_text))\n",
    "print(word_tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "   This is an example sentence demonstrating stop word filtration.\n",
      "   example sentence demonstrating stop word filtration .\n",
      "Words:\n",
      "   ['This', 'is', 'an', 'example', 'sentence', 'demonstrating', 'stop', 'word', 'filtration', '.']\n",
      "   ['example', 'sentence', 'demonstrating', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"This is an example sentence demonstrating stop word filtration.\"\n",
    "\n",
    "# NLTK's default list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_text = [word for word in words if not word.lower() in stop_words]\n",
    "\n",
    "print(\"Text: \")\n",
    "print(\"  \", text)\n",
    "print(\"  \", \" \".join(filtered_text))\n",
    "\n",
    "print(\"Words:\")\n",
    "print(\"  \", words)\n",
    "print(\"  \", filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "   The boys are playing football and one boy is scoring goals.\n",
      "   the boy are play footbal and one boy is score goal .\n",
      "Words:\n",
      "   ['The', 'boys', 'are', 'playing', 'football', 'and', 'one', 'boy', 'is', 'scoring', 'goals', '.']\n",
      "   ['the', 'boy', 'are', 'play', 'footbal', 'and', 'one', 'boy', 'is', 'score', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a new Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example text\n",
    "text = \"The boys are playing football and one boy is scoring goals.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform stemming on each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Text: \")\n",
    "print(\"  \", text)\n",
    "print(\"  \", \" \".join(stemmed_words))\n",
    "\n",
    "print(\"Words:\")\n",
    "print(\"  \", words)\n",
    "print(\"  \", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "   The boys are playing football and one boy is scoring goals.\n",
      "   The boy are playing football and one boy is scoring goal .\n",
      "Words:\n",
      "   ['The', 'boys', 'are', 'playing', 'football', 'and', 'one', 'boy', 'is', 'scoring', 'goals', '.']\n",
      "   ['The', 'boy', 'are', 'playing', 'football', 'and', 'one', 'boy', 'is', 'scoring', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a new WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example text\n",
    "text = \"The boys are playing football and one boy is scoring goals.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform lemmatization on each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Text: \")\n",
    "print(\"  \", text)\n",
    "print(\"  \", \" \".join(lemmatized_words))\n",
    "\n",
    "print(\"Words:\")\n",
    "print(\"  \", words)\n",
    "print(\"  \", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:\n",
      "   ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Tagged words:\n",
      "   [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "print(\"Words:\")\n",
    "print(\"  \", words)\n",
    "print(\"Tagged words:\")\n",
    "print(\"  \", tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      "  [('Hello', 'world'), ('world', ','), (',', 'this'), ('this', 'is'), ('is', 'a'), ('a', 'test'), ('test', 'sentence'), ('sentence', 'for'), ('for', 'generating'), ('generating', 'n-grams')]\n",
      "Trigrams:\n",
      "  [('Hello', 'world', ','), ('world', ',', 'this'), (',', 'this', 'is'), ('this', 'is', 'a'), ('is', 'a', 'test'), ('a', 'test', 'sentence'), ('test', 'sentence', 'for'), ('sentence', 'for', 'generating'), ('for', 'generating', 'n-grams')]\n",
      "4-grams:\n",
      "  [('Hello', 'world', ',', 'this'), ('world', ',', 'this', 'is'), (',', 'this', 'is', 'a'), ('this', 'is', 'a', 'test'), ('is', 'a', 'test', 'sentence'), ('a', 'test', 'sentence', 'for'), ('test', 'sentence', 'for', 'generating'), ('sentence', 'for', 'generating', 'n-grams')]\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Hello world, this is a test sentence for generating n-grams\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(tokens, n):\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Generate bigrams (n=2), trigrams (n=3), and 4-grams\n",
    "bigrams = generate_ngrams(tokens, 2)\n",
    "trigrams = generate_ngrams(tokens, 3)\n",
    "fourgrams = generate_ngrams(tokens, 4)\n",
    "\n",
    "print(\"Bigrams:\")\n",
    "print(f\"  {bigrams}\")\n",
    "print(\"Trigrams:\")\n",
    "print(f\"  {trigrams}\")\n",
    "print(\"4-grams:\")\n",
    "print(f\"  {fourgrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Category_Freq\n",
      "0    Apple              2\n",
      "1   Banana              2\n",
      "2    Apple              2\n",
      "3   Orange              2\n",
      "4   Banana              2\n",
      "5   Orange              2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data\n",
    "data = {'Category': ['Apple', 'Banana', 'Apple', 'Orange', 'Banana', 'Orange']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Frequency encoding\n",
    "frequency_encoding = df['Category'].value_counts().to_dict()\n",
    "df['Category_Freq'] = df['Category'].map(frequency_encoding)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category_Apple  Category_Banana  Category_Orange\n",
      "0            True            False            False\n",
      "1           False             True            False\n",
      "2            True            False            False\n",
      "3           False            False             True\n",
      "4           False             True            False\n",
      "5           False            False             True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Category': ['Apple', 'Banana', 'Apple', 'Orange', 'Banana', 'Orange']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df['Category'], prefix='Category')\n",
    "\n",
    "print(one_hot_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "TF-IDF(t,d)=TF(t,d)×IDF(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       blue    bright       can        in        is       see   shining  \\\n",
      "0  0.659191  0.000000  0.000000  0.000000  0.420753  0.000000  0.000000   \n",
      "1  0.000000  0.522109  0.000000  0.000000  0.522109  0.000000  0.000000   \n",
      "2  0.000000  0.321846  0.000000  0.504235  0.321846  0.000000  0.000000   \n",
      "3  0.000000  0.239102  0.374599  0.000000  0.000000  0.374599  0.374599   \n",
      "\n",
      "        sky       sun       the        we  \n",
      "0  0.519714  0.000000  0.343993  0.000000  \n",
      "1  0.000000  0.522109  0.426858  0.000000  \n",
      "2  0.397544  0.321846  0.526261  0.000000  \n",
      "3  0.000000  0.478204  0.390963  0.374599  \n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"The sky is blue.\",\n",
    "    \"The sun is bright.\",\n",
    "    \"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply the vectorizer to the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the names of the features (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the matrix to a dense array and put it in a DataFrame for better readability\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfidf_matrix.todense(), columns=feature_names)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
      " -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
      " -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767582  0.00423474\n",
      "  0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
      "  0.00794257  0.00865741 -0.00989575 -0.0067557   0.00133757  0.0064403\n",
      "  0.00737382  0.00551698  0.00766163 -0.00512557  0.00658441 -0.00410837\n",
      " -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
      "  0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033661\n",
      "  0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
      " -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
      "  0.0050456   0.00671352  0.00451564  0.00866716  0.00747497 -0.00108189\n",
      "  0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
      " -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194235  0.00608616\n",
      "  0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
      "  0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
      "  0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
      "  0.00864797 -0.00313511 -0.00633882  0.00987008]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"The quick brown fox jumps over the lazy dog\",\n",
    "             \"I love dogs and foxes\",\n",
    "             \"The dog is in the garden\"]\n",
    "\n",
    "# Tokenizing the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Training a Word2Vec model\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Getting the vector for a word\n",
    "vector = model.wv['fox']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
      "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
      "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
      "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
      "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
      "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
      " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
      " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
      "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
      "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
      "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
      " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
      " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
      "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
      "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
      " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
      " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
      " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
      " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
      " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
      " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
      " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
      "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "sentences = [\"I love machine learning\", \"I am exploring NLP\", \"I am a beginner in machine learning\"]\n",
    "\n",
    "# Tokenization\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(tokenized_sentences, min_count=1)\n",
    "\n",
    "# Access vector for one word\n",
    "print(model.wv['machine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.34165, -0.81267, 1.4513, 0.05914, -0.080801, 0.39567, 0.10064, -0.5468, -0.18887, 0.11364, -0.040956, -0.5637, -0.32191, 0.15968, -0.59756, -0.14571, -0.77074, 1.2955, -0.72002, -0.90818, 0.76644, 0.05346, -0.0031632, -0.15341, 0.22065, -1.191, -1.0775, -0.29768, 1.327, -0.51359, 2.6229, -0.67411, -0.82558, 0.14283, -0.014214, 0.90775, 0.66828, 0.48431, 0.1543, 0.26044, 1.0191, 0.015872, -0.75325, 0.58992, 0.4546, -0.19678, 0.42138, -0.43168, 0.11985, 0.14094]\n"
     ]
    }
   ],
   "source": [
    "def load_glove_model(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        model = {}\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = [float(val) for val in split_line[1:]]\n",
    "            model[word] = embedding\n",
    "        return model\n",
    "\n",
    "glove_model = load_glove_model('data/glove/glove.6B.50d.txt') # replace with your path to GloVe file\n",
    "\n",
    "# Access vector for one word\n",
    "print(glove_model['machine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.3493250e-04 -3.2697237e-05  1.5724794e-03 -4.3578114e-04\n",
      " -1.0922313e-03 -2.7957156e-03 -9.5741020e-04 -5.5972039e-04\n",
      " -1.2796433e-05 -9.3388435e-04  1.9502465e-03  6.0458242e-05\n",
      "  2.4036055e-04  8.6894573e-04  8.0683391e-04 -5.5166340e-04\n",
      " -1.0096406e-03  7.2244304e-04  1.0901949e-03  1.2013994e-03\n",
      " -1.0446154e-03  4.9545517e-04 -3.3387766e-04  7.2457740e-04\n",
      " -1.6949677e-03  1.7823363e-04  2.5758869e-03 -8.3602348e-04\n",
      " -6.9080963e-04  1.6026372e-03  3.0975667e-04  4.6462650e-04\n",
      "  4.1846634e-04  3.9506372e-04  8.9535628e-05  2.1438985e-03\n",
      "  3.5811459e-05  1.1164135e-03 -4.3014187e-04  2.8502289e-04\n",
      "  1.1015051e-03  4.1251557e-04  6.0182286e-04 -2.0248196e-03\n",
      " -9.8156133e-06 -1.2037809e-03 -8.1958412e-04  1.5580680e-04\n",
      " -1.5764225e-03 -7.0900656e-05  7.9386518e-04  2.8888283e-03\n",
      "  1.0871483e-03 -3.7539122e-04 -2.4249775e-03 -1.1455865e-03\n",
      "  5.2204775e-04 -4.4789197e-04 -3.2897203e-04 -3.6794288e-04\n",
      "  6.4575358e-04 -1.9342760e-05 -1.9213838e-04 -4.3491388e-04\n",
      "  5.4921664e-04  3.1576754e-05  1.0774737e-03  1.0391861e-03\n",
      "  1.0756503e-03 -2.1298048e-04 -1.0642108e-03 -4.6613091e-04\n",
      " -8.0313825e-04 -9.2194276e-04 -1.7931017e-04  1.8826789e-03\n",
      " -9.2135312e-04  2.0504894e-03  2.8354570e-04 -1.4566445e-04\n",
      " -1.0359038e-03  1.2284293e-03 -5.2366508e-05 -4.1363391e-04\n",
      " -2.3391123e-04 -1.8982262e-03 -1.7661571e-03  4.8737123e-04\n",
      " -9.7659940e-04 -8.0756412e-04  3.1865048e-04  1.4066245e-04\n",
      " -2.6308790e-05 -9.5032121e-04 -8.6737180e-04 -9.6187100e-04\n",
      " -4.2770608e-04 -1.3062268e-04 -1.9240823e-03  1.3650032e-03]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "sentences = [\"I love machine learning\", \"I am exploring NLP\", \"I am a beginner in machine learning\"]\n",
    "\n",
    "# Tokenization\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train the fastText model\n",
    "model = FastText(tokenized_sentences, min_count=1)\n",
    "\n",
    "# Access vector for one word\n",
    "print(model.wv['machine'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
