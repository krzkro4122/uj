{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./vae_img'):\n",
    "    os.mkdir('./vae_img')\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 21\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/rep/uj/.venv/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "reconstruction_function = nn.MSELoss(size_average=False)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 183.817139\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 50.561745\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 42.136742\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 39.315399\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 38.077408\n",
      "====> Epoch: 0 Average loss: 45.4789\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 36.771904\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 34.656021\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 35.463531\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 32.896961\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 34.887367\n",
      "====> Epoch: 1 Average loss: 35.0177\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 34.058094\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 34.952133\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 34.827469\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 34.519455\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 34.099075\n",
      "====> Epoch: 2 Average loss: 33.1806\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 32.473427\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 33.518040\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 32.398026\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 31.412209\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 31.952623\n",
      "====> Epoch: 3 Average loss: 32.2715\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 32.875320\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 32.333904\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 30.888727\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 31.224571\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 30.981623\n",
      "====> Epoch: 4 Average loss: 31.7591\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 31.275543\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 32.482201\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 31.471067\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 30.632069\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 31.309853\n",
      "====> Epoch: 5 Average loss: 31.3958\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 30.520845\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 31.631680\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 31.723761\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 29.897848\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 31.693960\n",
      "====> Epoch: 6 Average loss: 31.1377\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 30.479652\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 31.104271\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 29.995495\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 30.292479\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 31.594170\n",
      "====> Epoch: 7 Average loss: 30.9392\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 30.491114\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 30.885143\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 30.252319\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 31.442711\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 30.559450\n",
      "====> Epoch: 8 Average loss: 30.7678\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 30.157049\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 30.714170\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 31.492777\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 30.236309\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 30.915052\n",
      "====> Epoch: 9 Average loss: 30.6419\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 30.915438\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 30.944817\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 29.788630\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 30.323738\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 30.970592\n",
      "====> Epoch: 10 Average loss: 30.5077\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 31.564951\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 30.884106\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 32.030586\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 31.194983\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 29.635571\n",
      "====> Epoch: 11 Average loss: 30.3956\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 30.943455\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 30.147881\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 29.786449\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 30.917395\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 30.805443\n",
      "====> Epoch: 12 Average loss: 30.2853\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 31.324028\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 30.301691\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 29.290993\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 31.369171\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 30.171701\n",
      "====> Epoch: 13 Average loss: 30.2033\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 29.911415\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 30.093351\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 30.465225\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 29.507149\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 30.763954\n",
      "====> Epoch: 14 Average loss: 30.1422\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 28.949326\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 30.830999\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 30.004616\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 29.610346\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 29.429195\n",
      "====> Epoch: 15 Average loss: 30.0761\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 29.643019\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 28.868847\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 30.200905\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 29.887102\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 29.945274\n",
      "====> Epoch: 16 Average loss: 30.0030\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 29.662788\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 29.689734\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 30.230675\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 29.719002\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 29.651781\n",
      "====> Epoch: 17 Average loss: 29.9000\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 29.784752\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 29.475245\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 30.872021\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 30.482788\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 30.002420\n",
      "====> Epoch: 18 Average loss: 29.8503\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 31.371164\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 30.727316\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 29.622562\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 29.805367\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 29.797180\n",
      "====> Epoch: 19 Average loss: 29.7689\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 28.832132\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 29.568491\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 28.894230\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 29.579475\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 29.243195\n",
      "====> Epoch: 20 Average loss: 29.7052\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss = loss_function(recon_batch, img, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(img),\n",
    "                len(dataloader.dataset), 100. * batch_idx / len(dataloader), # type: ignore\n",
    "                loss.data.item() / len(img))) # type: ignore\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(dataloader.dataset))) # type: ignore\n",
    "    if epoch % 10 == 0:\n",
    "        save = to_img(recon_batch.cpu().data)\n",
    "        save_image(save, './vae_img/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './vae.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
